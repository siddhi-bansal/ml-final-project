{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data Preparation\n",
    "Our original dataset contains 150,346 entries of businesses recorded on Yelp, each record containing attributes such as ‘name’, ‘address’, ‘latitude’, ‘longitude’, ‘review_count’, with a labels column of ‘stars’.\n",
    "\n",
    "Since we want our model to focus on predicting the success of restaurants, we must only keep the businesses that are labeled as ‘Restaurants’. Additionally, we must drop the columns that are irrelevant and those that will bias/skew our models, such as ‘name’ and ‘business_id’. We are choosing to deal with NaNs by dropping all records with NaN values and further downsizing our dataset by dropping all businesses that are no longer open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data frame: (150346, 14)\n"
     ]
    }
   ],
   "source": [
    "# Read the data file\n",
    "\n",
    "df = pd.read_json('data/yelp_academic_dataset_business.json', lines=True)\n",
    "print(\"Shape of the data frame:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the modified data frame: (117618, 11)\n"
     ]
    }
   ],
   "source": [
    "# Drop all records with missing values and irrelevant columns\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=['name', 'address', 'city'])\n",
    "\n",
    "print(\"Shape of the modified data frame:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the modified data frame: (31357, 10)\n"
     ]
    }
   ],
   "source": [
    "# Keep only businesses that are restaurants\n",
    "df = df[df['categories'].str.contains('Restaurants')]\n",
    "\n",
    "# Keep only businesses that are still open (not permanently closed)\n",
    "df = df[df['is_open']==1]\n",
    "\n",
    "# Drop the is_open column (irrelevant)\n",
    "df = df.drop(columns='is_open')\n",
    "\n",
    "print(\"Shape of the modified data frame:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON data in attributes and hours columns to individual feature columns\n",
    "\n",
    "df = df.join(pd.json_normalize(df['attributes']))\n",
    "df = df.join(pd.json_normalize(df['hours']))\n",
    "\n",
    "# Drop the attributes and hours columns containing JSON data\n",
    "df = df.drop(columns=['attributes', 'hours'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse an hours string and return number of hours open\n",
    "# e.g. 10:00-21:00 -> 11 hours\n",
    "\n",
    "def parse_hours(day_hours_str):\n",
    "    if pd.isna(day_hours_str):\n",
    "        return 0\n",
    "    \n",
    "    time_endpoints = str(day_hours_str).split('-')\n",
    "\n",
    "    if time_endpoints[0] == time_endpoints[1]:\n",
    "        # 0:0-0:0\n",
    "        return 24\n",
    "    \n",
    "    start_time = time.strptime(time_endpoints[0], \"%H:%M\")\n",
    "    end_time = time.strptime(time_endpoints[1], \"%H:%M\")\n",
    "\n",
    "    # account for edge cases in data where we have 10-1, which is technically 10am-1am\n",
    "    et_hour = (24 + end_time.tm_hour) if end_time.tm_hour < start_time.tm_hour else end_time.tm_hour\n",
    "    \n",
    "    start_time_td = timedelta(hours=start_time.tm_hour, minutes=start_time.tm_min)\n",
    "    end_time_td = timedelta(hours=et_hour, minutes=end_time.tm_min)\n",
    "\n",
    "    duration = end_time_td - start_time_td\n",
    "\n",
    "    return duration.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature (total_open_hours) by combining all individual day hours\n",
    "\n",
    "total_hours_arr = []\n",
    "count_neg = 0\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "for ind in df.index:\n",
    "    total_hours = 0\n",
    "\n",
    "    for day in days:\n",
    "        day_hours_str = df[day][ind]\n",
    "        day_hours = parse_hours(day_hours_str)\n",
    "        total_hours += day_hours\n",
    "    \n",
    "    total_hours_arr.append(total_hours)\n",
    "\n",
    "df['total_open_hours'] = total_hours_arr\n",
    "\n",
    "# Drop all the individual day hours columns\n",
    "df = df.drop(columns=days)\n",
    "\n",
    "# TODO show example of total_open_hours before dropping days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all features except a handful\n",
    "\n",
    "df = df.filter(['total_open_hours', 'RestaurantsTakeOut', 'RestaurantsDelivery', 'Alcohol', 'latitude', 'longitude', 'stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with false\n",
    "\n",
    "df['RestaurantsTakeOut'] = df['RestaurantsTakeOut'].fillna('False')\n",
    "df['RestaurantsDelivery'] = df['RestaurantsDelivery'].fillna('False')\n",
    "df['Alcohol'] = df['Alcohol'].fillna('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert alcohol column to true/false values only\n",
    "\n",
    "def alcohol_tf(val):\n",
    "    if 'beer_and_wine' in val or 'full_bar' in val:\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "df['Alcohol_TF'] = df['Alcohol'].apply(alcohol_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all 'None' values with False\n",
    "df.replace('None', 'False', inplace=True)\n",
    "\n",
    "# Convert all string representations of T/F to boolean values\n",
    "df.replace({'True': True, 'False': False}, inplace=True)\n",
    "\n",
    "# Drop alcohol feature (already feature engineered it)\n",
    "df.drop(columns=['Alcohol'], inplace = True)\n",
    "\n",
    "# Rename alcohol T/F feature column\n",
    "df = df.rename(columns={'Alcohol_TF':'Alcohol'})\n",
    "\n",
    "# Reset the index to 0, 1, ... - it changed after all the drops and modifications\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KNN to find k nearest restaurants (by latitude/longitude)\n",
    "\n",
    "# Create new feature (avg_star_rating) averaging the star rating of the k nearest restaurants\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "location_df = df[['latitude', 'longitude', 'stars']]\n",
    "\n",
    "stars = location_df['stars']\n",
    "location_df = location_df.drop(columns='stars')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "location_df = pd.DataFrame(scaler.fit_transform(location_df), columns=location_df.columns)\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=51, n_jobs=-1)\n",
    "\n",
    "neigh.fit(location_df[['latitude', 'longitude']])\n",
    "\n",
    "distances, indices = neigh.kneighbors(location_df[['latitude', 'longitude']])\n",
    "\n",
    "for i in range(len(location_df)):\n",
    "    location_df.loc[i, 'avg_star_rating'] = stars.iloc[indices[i]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stacking feature to main df\n",
    "df['stack_1'] = location_df['avg_star_rating']\n",
    "\n",
    "# Drop latitude/longitude feature columns (no need anymore)\n",
    "df.drop(columns=['latitude', 'longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree Regressor for second stack as a feature in dataframe\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = DecisionTreeRegressor(random_state=0)\n",
    "X = df.loc[:, df.columns != 'stars']\n",
    "y = df.loc[:, 'stars']\n",
    "\n",
    "# Fitting the model based on max_depth = 4 because we have 4 features, ideally splitting on each one\n",
    "regressor = DecisionTreeRegressor(random_state=0, max_depth=4)\n",
    "regressor.fit(X, y) \n",
    "\n",
    "y_pred = regressor.predict(X) \n",
    "df['stack_2'] = y_pred\n",
    "\n",
    "# Reorder df columns\n",
    "df = df[['RestaurantsTakeOut', 'RestaurantsDelivery', 'Alcohol', 'total_open_hours', 'stack_1', 'stack_2', 'stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RestaurantsTakeOut</th>\n",
       "      <th>RestaurantsDelivery</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>total_open_hours</th>\n",
       "      <th>stack_1</th>\n",
       "      <th>stack_2</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.960784</td>\n",
       "      <td>3.927685</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>53.0</td>\n",
       "      <td>3.225490</td>\n",
       "      <td>3.176283</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.235294</td>\n",
       "      <td>3.176283</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.931373</td>\n",
       "      <td>3.927685</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.284314</td>\n",
       "      <td>3.265017</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RestaurantsTakeOut  RestaurantsDelivery  Alcohol  total_open_hours  \\\n",
       "0               False                False    False              23.0   \n",
       "1                True                 True     True              53.0   \n",
       "2               False                False    False             100.0   \n",
       "3                True                 True    False              60.0   \n",
       "4                True                False     True              58.0   \n",
       "\n",
       "    stack_1   stack_2  stars  \n",
       "0  3.960784  3.927685    4.0  \n",
       "1  3.225490  3.176283    2.0  \n",
       "2  3.235294  3.176283    1.5  \n",
       "3  3.931373  3.927685    4.0  \n",
       "4  3.284314  3.265017    2.5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final df for model looks like this (features and labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with all combinations of stacks/no stacks, so define variables for the same\n",
    "labels = df['stars']\n",
    "features_without_stacks = df.drop(columns=['stack_1', 'stack_2', 'stars'])\n",
    "features_stack1 = df.drop(columns=['stack_2', 'stars'])\n",
    "features_stack2 = df.drop(columns=['stack_1', 'stars'])\n",
    "features_all_stacks = df.drop(columns=['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of Linear Regression with features_without_stacks: -0.0007713364626037889\n",
      "R^2 of Linear Regression with features_stack1: 0.09931034573759065\n",
      "R^2 of Linear Regression with nfeatures_stack2: 0.10072682147360687\n",
      "R^2 of Linear Regression with features_all_stacks: 0.10077162630391703\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# features_without_stacks\n",
    "\n",
    "X = features_without_stacks\n",
    "y = labels\n",
    "reg = LinearRegression()\n",
    "\n",
    "# 10-fold cross validation on LinearRegression() Model\n",
    "scores = cross_val_score(reg, X, y, cv=10)\n",
    "print('R^2 of Linear Regression with features_without_stacks:', np.mean(scores))\n",
    "\n",
    "# features_stack1\n",
    "\n",
    "X = features_stack1\n",
    "y = labels\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(reg, X, y, cv=10)\n",
    "print('R^2 of Linear Regression with features_stack1:', np.mean(scores))\n",
    "\n",
    "# features_stack2\n",
    "\n",
    "X = features_stack2\n",
    "y = labels\n",
    "reg = LinearRegression()\n",
    "\n",
    "# 10-fold cross validation on LinearRegression() Model\n",
    "scores = cross_val_score(reg, X, y, cv=10)\n",
    "print('R^2 of Linear Regression with nfeatures_stack2:', np.mean(scores))\n",
    "\n",
    "# features_all_stacks\n",
    "\n",
    "X = features_all_stacks\n",
    "y = labels\n",
    "reg = LinearRegression()\n",
    "\n",
    "# 10-fold cross validation on LinearRegression() Model\n",
    "scores = cross_val_score(reg, X, y, cv=10)\n",
    "print('R^2 of Linear Regression with features_all_stacks:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of Decision Tree Regressor with features_without_stacks: -0.002657190982240931\n",
      "R^2 Decision Tree Regressor with features_stack1: 0.09791090772762898\n",
      "R^2 of Decision Tree Regressor with features_stack2: 0.09953694893616721\n",
      "R^2 of Decision Tree Regressor with features_all_stacks: 0.09795853583354919\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# features_without_stacks\n",
    "\n",
    "X = features_without_stacks\n",
    "y = labels\n",
    "reg_dt = DecisionTreeRegressor(random_state=0, max_depth=4)\n",
    "\n",
    "# 10-fold cross validation on LinearRegression() Model\n",
    "scores = cross_val_score(reg_dt, X, y, cv=10)\n",
    "print('R^2 of Decision Tree Regressor with features_without_stacks:', np.mean(scores))\n",
    "\n",
    "# features_stack1\n",
    "\n",
    "X = features_stack1\n",
    "y = labels\n",
    "reg_dt = DecisionTreeRegressor(random_state=0, max_depth=4)\n",
    "\n",
    "# 10-fold cross validation on LinearRegression() Model\n",
    "scores = cross_val_score(reg_dt, X, y, cv=10)\n",
    "print('R^2 Decision Tree Regressor with features_stack1:', np.mean(scores))\n",
    "\n",
    "# features_stack2\n",
    "\n",
    "X = features_stack2\n",
    "y = labels\n",
    "reg_dt = DecisionTreeRegressor(random_state=0, max_depth=4)\n",
    "\n",
    "# 10-fold cross validation on LinearRegression() Model\n",
    "scores = cross_val_score(reg_dt, X, y, cv=10)\n",
    "print('R^2 of Decision Tree Regressor with features_stack2:', np.mean(scores))\n",
    "\n",
    "# features_all_stacks\n",
    "\n",
    "X = features_all_stacks\n",
    "y = labels\n",
    "reg_dt = DecisionTreeRegressor(random_state=0, max_depth=4)\n",
    "\n",
    "# 10-fold cross validation on LinearRegression() Model\n",
    "scores = cross_val_score(reg_dt, X, y, cv=10)\n",
    "print('R^2 of Decision Tree Regressor with features_all_stacks:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of Neural Nets MLP Regressor features_without_stacks: -28.021152152437928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR^2 of Neural Nets MLP Regressor features_without_stacks:\u001b[39m\u001b[38;5;124m\"\u001b[39m, r2_score(predictions, labels))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# neural net training with features_stack1, 5-fold cross validation\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_search\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_stack1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR^2 of Neural Nets MLP Regressor with features_stack1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, r2_score(predictions, labels))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# neural net training with features_stack2, 5-fold cross validation\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1293\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m-> 1293\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1307\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1378\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[1;32m   1380\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Neural Nets Regressor\n",
    "\n",
    "# setting up neural network, with pipeline that scales the data\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "mlp_reg = MLPRegressor()\n",
    "scaler = StandardScaler()\n",
    "pipeline = Pipeline([('scaler', scaler), ('mlp', mlp_reg)])\n",
    "param_grid = {\n",
    "    'mlp__activation': ['logistic', 'tanh', 'relu']\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs = -1)\n",
    "\n",
    "# neural net training with features_without_stacks, 5-fold cross validation\n",
    "predictions = cross_val_predict(grid_search, features_without_stacks, labels, cv=5)\n",
    "print(\"R^2 of Neural Nets MLP Regressor features_without_stacks:\", r2_score(predictions, labels))\n",
    "\n",
    "# neural net training with features_stack1, 5-fold cross validation\n",
    "predictions = cross_val_predict(grid_search, features_stack1, labels, cv=5)\n",
    "print(\"R^2 of Neural Nets MLP Regressor with features_stack1:\", r2_score(predictions, labels))\n",
    "\n",
    "# neural net training with features_stack2, 5-fold cross validation\n",
    "predictions = cross_val_predict(grid_search, features_stack2, labels, cv=5)\n",
    "print(\"R^2 of Neural Nets MLP Regressor with features_stack2:\", r2_score(predictions, labels))\n",
    "\n",
    "# neural net training with features_all_stacks, 5-fold cross validation\n",
    "predictions = cross_val_predict(grid_search, features_all_stacks, labels, cv=5)\n",
    "print(\"R^2 of Neural Nets MLP Regressor with features_all_stacks:\", r2_score(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_bins = 9\n",
    "bin_labels = [f'Category_{i}' for i in range(num_bins)]\n",
    "labels_categorical = pd.cut(labels, bins=num_bins, labels=bin_labels)\n",
    "\n",
    "# Features Without Stacks, 5-fold cross validation\n",
    "knn = KNeighborsClassifier(n_neighbors=350)\n",
    "cv_scores = cross_val_score(knn, features_without_stacks, labels_categorical, cv=5)\n",
    "# Calculate the accuracy\n",
    "avg_score = np.mean(cv_scores) * 100.0\n",
    "print('Average accuracy of KNN Classifier with features_without_stacks: ' + str(avg_score) + \"%\")\n",
    "\n",
    "# With only Stack 1, 5-fold cross validation\n",
    "cv_scores = cross_val_score(knn, features_stack1, labels_categorical, cv=5)\n",
    "avg_score = np.mean(cv_scores) * 100.0\n",
    "print('Average accuracy of KNN Classifier with features_stack1: ' + str(avg_score) + \"%\")\n",
    "\n",
    "# With only Stack 2, 5-fold cross validation\n",
    "cv_scores = cross_val_score(knn, features_stack2, labels_categorical, cv=5)\n",
    "avg_score = np.mean(cv_scores) * 100.0\n",
    "print('Average accuracy of KNN Classifier with features_stack2: ' + str(avg_score) + \"%\")\n",
    "\n",
    "# With both Stack 1 and Stack 2, 5-fold cross validation\n",
    "cv_scores = cross_val_score(knn, features_all_stacks, labels_categorical, cv=5)\n",
    "avg_score = np.mean(cv_scores) * 100.0\n",
    "print('Average accuracy of KNN Classifier with features_all_stacks: ' + str(avg_score) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Convert labels from floats to categorical classes (for classification)\n",
    "# 1.0, 1.5, ..., 4.5, 5.0 --> 9 bins\n",
    "num_bins = 9\n",
    "bin_labels = [f'Category_{i}' for i in range(num_bins)]\n",
    "labels_categorical = pd.cut(labels, bins=num_bins, labels=bin_labels)\n",
    "\n",
    "clf_criterion = 'gini'\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=clf_criterion, random_state=0)\n",
    "\n",
    "# 5-fold cross validation\n",
    "scores = cross_val_score(clf, features_without_stacks, labels_categorical, cv=5)\n",
    "print(\"Accuracy of Decision Tree Classifier with features_without_stacks:\", scores.mean())\n",
    "\n",
    "# 5-fold cross validation\n",
    "scores = cross_val_score(clf, features_stack1, labels_categorical, cv=5)\n",
    "print(\"Accuracy of Decision Tree Classifier with features_stack1:\", scores.mean())\n",
    "\n",
    "# 5-fold cross validation\n",
    "scores = cross_val_score(clf, features_stack2, labels_categorical, cv=5)\n",
    "print(\"Accuracy of Decision Tree Classifier with features_stack2:\", scores.mean())\n",
    "\n",
    "# 5-fold cross validation\n",
    "scores = cross_val_score(clf, features_all_stacks, labels_categorical, cv=5)\n",
    "print(\"Accuracy of Decision Tree Classifier with features_all_stacks:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Nets Classifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline the data\n",
    "scaler = StandardScaler()\n",
    "nn = MLPClassifier(hidden_layer_sizes=30, activation='logistic')\n",
    "pipe = Pipeline(steps=[('scaler', scaler), \n",
    "                      ('mlp', nn)])\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid_search.fit(features_without_stacks, labels_categorical)\n",
    "\n",
    "# With no stacks, 5-fold cross validation\n",
    "cv_scores = cross_val_score(grid_search, features_without_stacks, labels_categorical, cv=5)\n",
    "print(\"Accuracy of Neural Nets MLP Classifier with features_without_stacks:\", str(cv_scores.mean() * 100) + \"%\")\n",
    "\n",
    "# With Stack 1 only, 5-fold cross validation\n",
    "grid_search.fit(features_stack1, labels_categorical)\n",
    "cv_scores = cross_val_score(grid_search, features_stack1, labels_categorical, cv=5)\n",
    "print(\"Accuracy of Neural Nets MLP Classifier with features_stack1:\", str(cv_scores.mean() * 100) + \"%\")\n",
    "\n",
    "# With Stack 2 only, 5-fold cross validation\n",
    "grid_search.fit(features_stack2, labels_categorical)\n",
    "cv_scores = cross_val_score(grid_search, features_stack2, labels_categorical, cv=5)\n",
    "print(\"Accuracy of Neural Nets MLP Classifier with features_stack2:\", str(cv_scores.mean() * 100) + \"%\")\n",
    "\n",
    "# With Stacks 1 and 2, 5-fold cross validation\n",
    "grid_search.fit(features_all_stacks, labels_categorical)\n",
    "cv_scores = cross_val_score(grid_search, features_all_stacks, labels_categorical, cv=5)\n",
    "print(\"Accuracy of Neural Nets MLP Classifier with features_all_stacks:\", str(cv_scores.mean() * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import random\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter(data, labels, numPoints = 300):\n",
    "\n",
    "    numEntries = data.shape[0]\n",
    "    start = random.randint(0, numEntries - numPoints)\n",
    "    end = start + numPoints\n",
    "    data = data.iloc[start:end, :]\n",
    "    labels = labels.iloc[start:end]\n",
    "    \n",
    "    mds = MDS(n_components=2)\n",
    "    mds_data = mds.fit_transform(data)\n",
    "    plt.scatter(mds_data[:, 0], mds_data[:, 1], c=labels, s=50)\n",
    "    plt.show()\n",
    "\n",
    "test_curr_start = 0\n",
    "test_curr_end = int(len(features_stack2) / 10)\n",
    "increment = int(len(features_stack2) / 10)\n",
    "mse = 0\n",
    "total_sh_score = 0\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    \n",
    "    # partition data into train_set and test_set\n",
    "    a = features_stack2.iloc[:test_curr_start, :].values\n",
    "    b = features_stack2.iloc[test_curr_end:, :].values\n",
    "    X_train = np.concatenate((a, b))\n",
    "    X_test = features_stack2.iloc[test_curr_start:test_curr_end, :].values\n",
    "    a = labels.iloc[:test_curr_start]\n",
    "    b = labels.iloc[test_curr_end:]\n",
    "    y_train = np.concatenate((a, b))\n",
    "    y_test = labels.iloc[test_curr_start:test_curr_end].values\n",
    "\n",
    "    kmeans = KMeans(n_clusters=9)\n",
    "    curr_clustering = kmeans.fit_predict(X_train)\n",
    "\n",
    "    sh_score = silhouette_score(X_train, curr_clustering)\n",
    "    total_sh_score += sh_score\n",
    "    print(f\"\\tsilhouette score: {sh_score}\")\n",
    "    # scatter(pd.DataFrame(X_train), pd.Series(curr_clustering)) # uncomment to see clustering for 300 random data points\n",
    "\n",
    "    trained_df = pd.DataFrame(X_train)\n",
    "    trained_df['k_means_cluster'] = curr_clustering\n",
    "    trained_df['stars'] = y_train\n",
    "    \n",
    "    # calculate average star prediction for each cluster\n",
    "    cluster_preds = trained_df.groupby('k_means_cluster', as_index=False)['stars'].mean()['stars']\n",
    "    pred_clustering = kmeans.predict(X_test)\n",
    "\n",
    "    predictions = []\n",
    "    for i in pred_clustering:\n",
    "        predictions.append(cluster_preds[i])\n",
    "\n",
    "    curr_mse = mean_squared_error(y_test, predictions)\n",
    "    print(\"\\tcurr_mse:\", curr_mse)\n",
    "    mse += curr_mse\n",
    "    test_curr_start += increment\n",
    "    test_curr_end += increment\n",
    "    \n",
    "print(\"\\nAverage sh score\", total_sh_score/10)\n",
    "print(\"Average mse\", mse/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final predictions for each cluster, trained on features_stack2\n",
    "kmeans = KMeans(n_clusters=9)\n",
    "curr_clustering = kmeans.fit_predict(features_stack2)\n",
    "\n",
    "# calculate average star prediction for each cluster\n",
    "cluster_preds = trained_df.groupby('k_means_cluster', as_index=False)['stars'].mean()['stars']\n",
    "\n",
    "\n",
    "# The star prediction for a new datapoint will be made based on the cluster that the new datapoint is in. \n",
    "# cluster_preds stores star predictions for each cluser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
