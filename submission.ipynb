{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data Preparation\n",
    "Our original dataset contains 150,346 entries of businesses recorded on Yelp, each record containing attributes such as ‘name’, ‘address’, ‘latitude’, ‘longitude’, ‘review_count’, with a labels column of ‘stars’.\n",
    "\n",
    "Since we want our model to focus on predicting the success of restaurants, we must only keep the businesses that are labeled as ‘Restaurants’. Additionally, we must drop the columns that are irrelevant and those that will bias/skew our models, such as ‘name’ and ‘business_id’. We are choosing to deal with NaNs by dropping all records with NaN values and further downsizing our dataset by dropping all businesses that are no longer open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data frame: (150346, 14)\n"
     ]
    }
   ],
   "source": [
    "# Read the data file\n",
    "\n",
    "df = pd.read_json('data/yelp_academic_dataset_business.json', lines=True)\n",
    "print(\"Shape of the data frame:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the modified data frame: (117618, 11)\n"
     ]
    }
   ],
   "source": [
    "# Drop all records with missing values and irrelevant columns\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=['name', 'address', 'city'])\n",
    "\n",
    "print(\"Shape of the modified data frame:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the modified data frame: (31357, 10)\n"
     ]
    }
   ],
   "source": [
    "# Keep only businesses that are restaurants\n",
    "df = df[df['categories'].str.contains('Restaurants')]\n",
    "\n",
    "# Keep only businesses that are still open (not permanently closed)\n",
    "df = df[df['is_open']==1]\n",
    "\n",
    "# Drop the is_open column (irrelevant)\n",
    "df = df.drop(columns='is_open')\n",
    "\n",
    "print(\"Shape of the modified data frame:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON data in attributes and hours columns to individual feature columns\n",
    "\n",
    "df = df.join(pd.json_normalize(df['attributes']))\n",
    "df = df.join(pd.json_normalize(df['hours']))\n",
    "\n",
    "# Drop the attributes and hours columns containing JSON data\n",
    "df = df.drop(columns=['attributes', 'hours'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse an hours string and return number of hours open\n",
    "# e.g. 10:00-21:00 -> 11 hours\n",
    "\n",
    "def parse_hours(day_hours_str):\n",
    "    if pd.isna(day_hours_str):\n",
    "        return 0\n",
    "    \n",
    "    time_endpoints = str(day_hours_str).split('-')\n",
    "\n",
    "    if time_endpoints[0] == time_endpoints[1]:\n",
    "        # 0:0-0:0\n",
    "        return 0\n",
    "    \n",
    "    start_time = time.strptime(time_endpoints[0], \"%H:%M\")\n",
    "    end_time = time.strptime(time_endpoints[1], \"%H:%M\")\n",
    "\n",
    "    # account for edge cases in data where we have 10-1, which is technically 10am-1am\n",
    "    et_hour = (24 + end_time.tm_hour) if end_time.tm_hour < start_time.tm_hour else end_time.tm_hour\n",
    "    \n",
    "    start_time_td = timedelta(hours=start_time.tm_hour, minutes=start_time.tm_min)\n",
    "    end_time_td = timedelta(hours=et_hour, minutes=end_time.tm_min)\n",
    "\n",
    "    duration = end_time_td - start_time_td\n",
    "\n",
    "    return duration.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature (total_open_hours) by combining all individual day hours\n",
    "\n",
    "total_hours_arr = []\n",
    "count_neg = 0\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "for ind in df.index:\n",
    "    total_hours = 0\n",
    "\n",
    "    for day in days:\n",
    "        day_hours_str = df[day][ind]\n",
    "        day_hours = parse_hours(day_hours_str)\n",
    "        total_hours += day_hours\n",
    "    \n",
    "    total_hours_arr.append(total_hours)\n",
    "\n",
    "df['total_open_hours'] = total_hours_arr\n",
    "\n",
    "# Drop all the individual day hours columns\n",
    "df = df.drop(columns=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "# TODO show example of total_open_hours before dropping days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all features except a handful\n",
    "\n",
    "df = df.filter(['total_open_hours', 'RestaurantsTakeOut', 'RestaurantsDelivery', 'Alcohol', 'latitude', 'longitude', 'stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with false\n",
    "\n",
    "df['RestaurantsTakeOut'] = df['RestaurantsTakeOut'].fillna('False')\n",
    "df['RestaurantsDelivery'] = df['RestaurantsDelivery'].fillna('False')\n",
    "df['Alcohol'] = df['Alcohol'].fillna('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert alcohol column to true/false values only\n",
    "\n",
    "def alcohol_tf(val):\n",
    "    if 'beer_and_wine' in val or 'full_bar' in val:\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "df['Alcohol_TF'] = df['Alcohol'].apply(alcohol_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all 'None' values with False\n",
    "df.replace('None', 'False', inplace=True)\n",
    "\n",
    "# Convert all string representations of T/F to boolean values\n",
    "df.replace({'True': True, 'False': False}, inplace=True)\n",
    "\n",
    "# Drop alcohol feature (already feature engineered it)\n",
    "df.drop(columns=['Alcohol'], inplace = True)\n",
    "\n",
    "# Rename alcohol T/F feature column\n",
    "df = df.rename(columns={'Alcohol_TF':'Alcohol'})\n",
    "\n",
    "# Reset the index to 0, 1, ... - it changed after all the drops and modifications\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KNN to find k nearest restaurants (by latitude/longitude)\n",
    "# Create new feature (avg_star_rating) averaging the star rating of the k nearest restaurants\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "location_df = df[['latitude', 'longitude', 'stars']]\n",
    "\n",
    "stars = location_df['stars']\n",
    "location_df = location_df.drop(columns='stars')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "location_df = pd.DataFrame(scaler.fit_transform(location_df), columns=location_df.columns)\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=51, n_jobs=-1)\n",
    "\n",
    "neigh.fit(location_df[['latitude', 'longitude']])\n",
    "\n",
    "distances, indices = neigh.kneighbors(location_df[['latitude', 'longitude']])\n",
    "\n",
    "for i in range(len(location_df)):\n",
    "    location_df.loc[i, 'avg_star_rating'] = stars.iloc[indices[i]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stacking feature to main df\n",
    "df['stack_1'] = location_df['avg_star_rating']\n",
    "\n",
    "# Drop latitude/longitude feature columns (no need anymore)\n",
    "df.drop(columns=['latitude', 'longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree Regressor for second stack as a feature in dataframe\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = DecisionTreeRegressor(random_state=0)\n",
    "X = df.loc[:, df.columns != 'stars']\n",
    "y = df.loc[:, 'stars']\n",
    "\n",
    "# fitting the model based on max_depth = 4 because we have 4 features, splitting on them\n",
    "regressor = DecisionTreeRegressor(random_state=0, max_depth=4)\n",
    "regressor.fit(X, y) \n",
    "\n",
    "y_pred = regressor.predict(X) \n",
    "df['stack_2'] = y_pred\n",
    "\n",
    "# Reorder df columns\n",
    "df = df[['RestaurantsTakeOut', 'RestaurantsDelivery', 'Alcohol', 'total_open_hours', 'stack_1', 'stack_2', 'stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RestaurantsTakeOut</th>\n",
       "      <th>RestaurantsDelivery</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>total_open_hours</th>\n",
       "      <th>stack_1</th>\n",
       "      <th>stack_2</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.960784</td>\n",
       "      <td>3.927685</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>53.0</td>\n",
       "      <td>3.225490</td>\n",
       "      <td>3.176283</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.235294</td>\n",
       "      <td>3.176283</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.931373</td>\n",
       "      <td>3.927685</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.284314</td>\n",
       "      <td>3.265017</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RestaurantsTakeOut  RestaurantsDelivery  Alcohol  total_open_hours  \\\n",
       "0               False                False    False              23.0   \n",
       "1                True                 True     True              53.0   \n",
       "2               False                False    False             100.0   \n",
       "3                True                 True    False              36.0   \n",
       "4                True                False     True              34.0   \n",
       "\n",
       "    stack_1   stack_2  stars  \n",
       "0  3.960784  3.927685    4.0  \n",
       "1  3.225490  3.176283    2.0  \n",
       "2  3.235294  3.176283    1.5  \n",
       "3  3.931373  3.927685    4.0  \n",
       "4  3.284314  3.265017    2.5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final df for model looks like this\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with all combinations of stacks/no stacks, so define variables for the same\n",
    "labels = df['stars']\n",
    "features_without_stacks = df.drop(columns=['stack_1', 'stack_2', 'stars'])\n",
    "features_stack1 = df.drop(columns=['stack_2', 'stars'])\n",
    "features_stack2 = df.drop(columns=['stack_1', 'stars'])\n",
    "features_all_stacks = df.drop(columns=['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_without_stacks\n",
      "R2 using cv : -0.0005328556723235955\n",
      "R2 score using split:  0.0001536307443901208\n",
      "Intercept: 3.535930256296494\n",
      "features_stack1\n",
      "R2 using cv : 0.09948161608745322\n",
      "R2 score using split:  0.09799534075970584\n",
      "Intercept: -0.049912853603985674\n",
      "features_stack2\n",
      "R2 using cv : 0.10087005758208786\n",
      "R2 score using split:  0.09998361102449327\n",
      "Intercept: -0.013997160258973462\n",
      "features_all_stacks\n",
      "R2 using cv : 0.10089813189404132\n",
      "R2 score using split:  0.10390456733660092\n",
      "Intercept: -0.009543346771907935\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "# features_without_stacks\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print(\"features_without_stacks\")\n",
    "\n",
    "X = features_without_stacks\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(reg, X, y, cv=10)\n",
    "# print(scores)\n",
    "print('R2 using cv :',np.mean(scores))\n",
    "\n",
    "print('R2 score using split: ', reg.score(X_test, y_test))\n",
    "print('Intercept:', reg.intercept_)\n",
    "\n",
    "# features_stack1\n",
    "\n",
    "print(\"features_stack1\")\n",
    "\n",
    "X = features_stack1\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(reg, X, y, cv=10)\n",
    "# print(scores)\n",
    "print('R2 using cv :',np.mean(scores))\n",
    "\n",
    "print('R2 score using split: ', reg.score(X_test, y_test))\n",
    "print('Intercept:', reg.intercept_)\n",
    "\n",
    "# features_stack2\n",
    "\n",
    "print(\"features_stack2\")\n",
    "\n",
    "X = features_stack2\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(reg, X, y, cv=10)\n",
    "# print(scores)\n",
    "print('R2 using cv :',np.mean(scores))\n",
    "\n",
    "print('R2 score using split: ', reg.score(X_test, y_test))\n",
    "print('Intercept:', reg.intercept_)\n",
    "\n",
    "# features_all_stacks\n",
    "\n",
    "print(\"features_all_stacks\")\n",
    "\n",
    "X = features_all_stacks\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(reg, X, y, cv=10)\n",
    "# print(scores)\n",
    "print('R2 using cv :',np.mean(scores))\n",
    "\n",
    "print('R2 score using split: ', reg.score(X_test, y_test))\n",
    "print('Intercept:', reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_without_stacks\n",
      "Accuracy : -0.004579498809097049\n",
      "features_stack1\n",
      "Accuracy : 0.0976081543167847\n",
      "features_stack2\n",
      "Accuracy : 0.09912158622637697\n",
      "features_all_stacks\n",
      "Accuracy : 0.09781835673103907\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# features_without_stacks\n",
    "\n",
    "print(\"features_without_stacks\")\n",
    "\n",
    "X = features_without_stacks\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "reg_dt = DecisionTreeRegressor(random_state=0, max_depth=4).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(reg_dt, X, y, cv=10)\n",
    "print('Accuracy :', np.mean(scores))\n",
    "\n",
    "# features_stack1\n",
    "\n",
    "print(\"features_stack1\")\n",
    "\n",
    "X = features_stack1\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "reg_dt = DecisionTreeRegressor(random_state=0, max_depth=4).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(reg_dt, X, y, cv=10)\n",
    "print('Accuracy :',np.mean(scores))\n",
    "\n",
    "# features_stack2\n",
    "print(\"features_stack2\")\n",
    "\n",
    "X = features_stack2\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "reg_dt = DecisionTreeRegressor(random_state=0, max_depth=4).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(reg_dt, X, y, cv=10)\n",
    "print('Accuracy :',np.mean(scores))\n",
    "\n",
    "# features_all_stacks\n",
    "print(\"features_all_stacks\")\n",
    "\n",
    "X = features_all_stacks\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "reg_dt = DecisionTreeRegressor(random_state=0, max_depth=4).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(reg_dt, X, y, cv=10)\n",
    "print('Accuracy :',np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Neural Net w/ features_without_stacks: -41.38730180953108\n",
      "Accuracy of the Neural Net w/ features_stack1: -8.420935270135454\n",
      "Accuracy of the Neural Net w/ features_stack2: -8.043436463686998\n",
      "Accuracy of the Neural Net w/ features_all_stacks: -7.577177070368336\n"
     ]
    }
   ],
   "source": [
    "# Neural Nets Regressor\n",
    "\n",
    "# setting up neural network, with pipeline that scales the data\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "mlp_reg = MLPRegressor()\n",
    "scaler = StandardScaler()\n",
    "pipeline = Pipeline([('scaler', scaler), ('mlp', mlp_reg)])\n",
    "param_grid = {\n",
    "    'mlp__activation': ['logistic', 'tanh', 'relu']\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs = -1)\n",
    "\n",
    "# neural net training with features_without_stacks\n",
    "predictions = cross_val_predict(grid_search, features_without_stacks, labels, cv=5)\n",
    "print(\"Accuracy of the Neural Net w/ features_without_stacks:\", r2_score(predictions, labels))\n",
    "\n",
    "# neural net training with features_stack1\n",
    "predictions = cross_val_predict(grid_search, features_stack1, labels, cv=5)\n",
    "print(\"Accuracy of the Neural Net w/ features_stack1:\", r2_score(predictions, labels))\n",
    "\n",
    "# neural net training with features_stack2\n",
    "predictions = cross_val_predict(grid_search, features_stack2, labels, cv=5)\n",
    "print(\"Accuracy of the Neural Net w/ features_stack2:\", r2_score(predictions, labels))\n",
    "\n",
    "# neural net training with features_all_stacks\n",
    "predictions = cross_val_predict(grid_search, features_all_stacks, labels, cv=5)\n",
    "print(\"Accuracy of the Neural Net w/ features_all_stacks:\", r2_score(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. accuracy, no stacks: 25.920959148688976%\n",
      "Avg. accuracy, stack 1: 27.09761578646767%\n",
      "Avg. accuracy, stack 2: 27.113584071804453%\n",
      "Avg. accuracy, both stacks: 27.027447978872623%\n"
     ]
    }
   ],
   "source": [
    "# KNN Classifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_bins = 9\n",
    "bin_labels = [f'Category_{i}' for i in range(num_bins)]\n",
    "labels_categorical = pd.cut(labels, bins=num_bins, labels=bin_labels)\n",
    "\n",
    "# Features Without Stacks\n",
    "knn = KNeighborsClassifier(n_neighbors=350)\n",
    "cv_scores = cross_val_score(knn, features_without_stacks, labels_categorical, cv=5)\n",
    "# Calculate the accuracy\n",
    "avg_score = np.mean(cv_scores) * 100.0\n",
    "print('Avg. accuracy, no stacks: ' + str(avg_score) + \"%\")\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "# param_grid = {'n_neighbors': [1, 101]}\n",
    "# param_grid = {'n_neighbors': np.arange(1, 101)}\n",
    "# knn_gs = GridSearchCV(knn, param_grid, cv=5)\n",
    "# knn_gs.fit(features_stack1, labels_categorical)\n",
    "# print('Best k-value: ' + str(knn_gs.best_params_))\n",
    "\n",
    "# best so far: 18, 41, 99, 101, 198, 249\n",
    "\n",
    "\n",
    "# With only Stack 1\n",
    "cv_scores = cross_val_score(knn, features_stack1, labels_categorical, cv=5)\n",
    "avg_score = np.mean(cv_scores) * 100.0\n",
    "print('Avg. accuracy, stack 1: ' + str(avg_score) + \"%\")\n",
    "\n",
    "# best so far: 200, 350\n",
    "\n",
    "# With only Stack 2\n",
    "cv_scores = cross_val_score(knn, features_stack2, labels_categorical, cv=5)\n",
    "avg_score = np.mean(cv_scores) * 100.0\n",
    "print('Avg. accuracy, stack 2: ' + str(avg_score) + \"%\")\n",
    "\n",
    "\n",
    "# With both Stack 1 and Stack 2\n",
    "cv_scores = cross_val_score(knn, features_all_stacks, labels_categorical, cv=5)\n",
    "avg_score = np.mean(cv_scores) * 100.0\n",
    "print('Avg. accuracy, both stacks: ' + str(avg_score) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without stacks: 0.2531887755102041\n",
      "Accuracy with stack1: 0.2456951530612245\n",
      "Accuracy with stack2: 0.25605867346938777\n",
      "Accuracy with all stacks: 0.2498405612244898\n",
      "\n",
      "With 5-fold cross validation:\n",
      "Accuracy without stacks: 0.2548728567930122\n",
      "Accuracy with stack1: 0.24792054818259626\n",
      "Accuracy with stack2: 0.2541392197725846\n",
      "Accuracy with all stacks: 0.24747411452621235\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert labels from floats to categorical classes (for classification)\n",
    "# 1.0, 1.5, ..., 4.5, 5.0 --> 9 bins\n",
    "num_bins = 9\n",
    "bin_labels = [f'Category_{i}' for i in range(num_bins)]\n",
    "labels_categorical = pd.cut(labels, bins=num_bins, labels=bin_labels)\n",
    "\n",
    "clf_criterion = 'gini'\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_without_stacks, labels_categorical, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=clf_criterion, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy without stacks:\", accuracy_score(Y_test, Y_pred))\n",
    "\n",
    "# Decision Tree Classifier with stack1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_stack1, labels_categorical, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=clf_criterion, random_state=0)\n",
    "clf = DecisionTreeClassifier(criterion=clf_criterion, random_state=0)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy with stack1:\", accuracy_score(Y_test, Y_pred))\n",
    "\n",
    "# Decision Tree Classifier with stack2\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_stack2, labels_categorical, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=clf_criterion, random_state=0)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy with stack2:\", accuracy_score(Y_test, Y_pred))\n",
    "\n",
    "# Decision Tree Classifier with all stacks\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_all_stacks, labels_categorical, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=clf_criterion, random_state=0)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy with all stacks:\", accuracy_score(Y_test, Y_pred))\n",
    "\n",
    "# ------ NOW WITH CROSS VALIDATION ------ #\n",
    "\n",
    "print()\n",
    "print(\"With 5-fold cross validation:\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=clf_criterion, random_state=0)\n",
    "\n",
    "scores = cross_val_score(clf, features_without_stacks, labels_categorical, cv=5)\n",
    "print(\"Accuracy without stacks:\", scores.mean())\n",
    "\n",
    "scores = cross_val_score(clf, features_stack1, labels_categorical, cv=5)\n",
    "print(\"Accuracy with stack1:\", scores.mean())\n",
    "\n",
    "scores = cross_val_score(clf, features_stack2, labels_categorical, cv=5)\n",
    "print(\"Accuracy with stack2:\", scores.mean())\n",
    "\n",
    "scores = cross_val_score(clf, features_all_stacks, labels_categorical, cv=5)\n",
    "print(\"Accuracy with all stacks:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy, no stacks:  26.909479556852244%\n",
      "Average accuracy, stack 1 only:  27.12315700877704%\n",
      "Average accuracy, stack 2 only:  27.0434315190755%\n",
      "Average accuracy, both stacks:  26.947733676072883%\n"
     ]
    }
   ],
   "source": [
    "# Neural Nets Classifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline the data\n",
    "scaler = StandardScaler()\n",
    "nn = MLPClassifier(hidden_layer_sizes=30, activation='logistic')\n",
    "pipe = Pipeline(steps=[('scaler', scaler), \n",
    "                      ('mlp', nn)])\n",
    "\n",
    "# param_grid = {\n",
    "# #     'nn__hidden_layer_sizes': [(i,) for i in range(20, 71, 10)],\n",
    "#     'nn__activation': ['logistic', 'tanh', 'relu']\n",
    "# }\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid_search.fit(features_without_stacks, labels_categorical)\n",
    "# print(\"Best parameters found by grid search:\")\n",
    "# print(grid_search.best_params_)\n",
    "\n",
    "# With no stacks\n",
    "cv_scores = cross_val_score(grid_search, features_without_stacks, labels_categorical, cv=5)\n",
    "print(\"Average accuracy, no stacks: \", str(cv_scores.mean() * 100) + \"%\")\n",
    "\n",
    "# With Stack 1 only\n",
    "grid_search.fit(features_stack1, labels_categorical)\n",
    "cv_scores = cross_val_score(grid_search, features_stack1, labels_categorical, cv=5)\n",
    "print(\"Average accuracy, stack 1 only: \", str(cv_scores.mean() * 100) + \"%\")\n",
    "\n",
    "# With Stack 2 only\n",
    "grid_search.fit(features_stack2, labels_categorical)\n",
    "cv_scores = cross_val_score(grid_search, features_stack2, labels_categorical, cv=5)\n",
    "print(\"Average accuracy, stack 2 only: \", str(cv_scores.mean() * 100) + \"%\")\n",
    "\n",
    "# With Stacks 1 and 2\n",
    "grid_search.fit(features_all_stacks, labels_categorical)\n",
    "cv_scores = cross_val_score(grid_search, features_all_stacks, labels_categorical, cv=5)\n",
    "print(\"Average accuracy, both stacks: \", str(cv_scores.mean() * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "\tsilhouette score: 0.9261190105488161\n",
      "\tcurr_mse: 0.74362096482838\n",
      "Fold 2:\n",
      "\tsilhouette score: 0.9252173574789019\n",
      "\tcurr_mse: 0.7224076662081345\n",
      "Fold 3:\n",
      "\tsilhouette score: 0.8770877722440582\n",
      "\tcurr_mse: 0.7168845588530631\n",
      "Fold 4:\n",
      "\tsilhouette score: 0.8769091732901332\n",
      "\tcurr_mse: 0.7260861347019418\n",
      "Fold 5:\n",
      "\tsilhouette score: 0.8769106429930522\n",
      "\tcurr_mse: 0.7182482479677185\n",
      "Fold 6:\n",
      "\tsilhouette score: 0.8734139387259885\n",
      "\tcurr_mse: 0.7802842459950118\n",
      "Fold 7:\n",
      "\tsilhouette score: 0.8746193177645922\n",
      "\tcurr_mse: 0.7254716981775564\n",
      "Fold 8:\n",
      "\tsilhouette score: 0.878180079158142\n",
      "\tcurr_mse: 0.7323729981101268\n",
      "Fold 9:\n",
      "\tsilhouette score: 0.874962757518313\n",
      "\tcurr_mse: 0.7459960328828217\n",
      "Fold 10:\n",
      "\tsilhouette score: 0.8717100700080901\n",
      "\tcurr_mse: 0.7445906266908601\n",
      "\n",
      "average sh score 0.8855130119730088\n",
      "average mse 0.7355963174415614\n"
     ]
    }
   ],
   "source": [
    "# K-Means Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import random\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter(data, labels, numPoints = 300):\n",
    "\n",
    "    numEntries = data.shape[0]\n",
    "    start = random.randint(0, numEntries - numPoints)\n",
    "    end = start + numPoints\n",
    "    data = data.iloc[start:end, :]\n",
    "    labels = labels.iloc[start:end]\n",
    "    \n",
    "    mds = MDS(n_components=2)\n",
    "    mds_data = mds.fit_transform(data)\n",
    "    plt.scatter(mds_data[:, 0], mds_data[:, 1], c=labels, s=50)\n",
    "    plt.show()\n",
    "\n",
    "test_curr_start = 0\n",
    "test_curr_end = int(len(features_all_stacks) / 10)\n",
    "increment = int(len(features_all_stacks) / 10)\n",
    "mse = 0\n",
    "total_sh_score = 0\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    # partition data into train_set and test_set\n",
    "    a = features_all_stacks.iloc[:test_curr_start, :].values\n",
    "    b = features_all_stacks.iloc[test_curr_end:, :].values\n",
    "    X_train = np.concatenate((a, b))\n",
    "    # X_train = features_all_stacks.iloc[:test_curr_start, :].values + features_all_stacks.iloc[test_curr_end:, :].values\n",
    "    X_test = features_all_stacks.iloc[test_curr_start:test_curr_end, :].values\n",
    "    a = labels.iloc[:test_curr_start]\n",
    "    b = labels.iloc[test_curr_end:]\n",
    "    y_train = np.concatenate((a, b))\n",
    "    y_test = labels.iloc[test_curr_start:test_curr_end].values\n",
    "\n",
    "    kmeans = KMeans(n_clusters=9)\n",
    "    curr_clustering = kmeans.fit_predict(X_train)\n",
    "\n",
    "    sh_score = silhouette_score(X_train, curr_clustering)\n",
    "    total_sh_score += sh_score\n",
    "    # print(curr_clustering)\n",
    "    print(f\"\\tsilhouette score: {sh_score}\")\n",
    "    # scatter(pd.DataFrame(X_train), pd.Series(curr_clustering))\n",
    "\n",
    "    trained_df = pd.DataFrame(X_train)\n",
    "    trained_df['k_means_cluster'] = curr_clustering\n",
    "    trained_df['stars'] = y_train\n",
    "    # print(trained_df['stars'])\n",
    "    \n",
    "    # calculate average star prediction for each cluster\n",
    "    # print(trained_df.groupby('k_means_cluster', as_index=False)['stars'].mean())\n",
    "\n",
    "    cluster_preds = trained_df.groupby('k_means_cluster', as_index=False)['stars'].mean()['stars']\n",
    "    pred_clustering = kmeans.predict(X_test)\n",
    "    # print(cluster_preds)\n",
    "    predictions = []\n",
    "    for i in pred_clustering:\n",
    "        predictions.append(cluster_preds[i])\n",
    "    # print(y_test)\n",
    "    # print(predictions)\n",
    "    curr_mse = mean_squared_error(y_test, predictions)\n",
    "    print(\"\\tcurr_mse:\", curr_mse)\n",
    "    mse += curr_mse\n",
    "    test_curr_start += increment\n",
    "    test_curr_end += increment\n",
    "    \n",
    "print(\"\\naverage sh score\", total_sh_score/10)\n",
    "print(\"average mse\", mse/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
